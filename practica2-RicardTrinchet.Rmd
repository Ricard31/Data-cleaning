---
  title: "Pr√°ctica 2"
  author: "Ricard Trinchet Arnejo"
  date: "11th of June 2019"
  header-includes:
  - \usepackage[spanish]{babel}
  - \usepackage{float}
  output:
    pdf_document:
      fig_caption: yes
      keep_tex: yes
      number_sections: true
      toc: yes
      toc_depth: 3

---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.pos = 'H')
```

-----------------------




## packages

These are the packages that will be needed in this project:


```{r ,message=FALSE, warning=FALSE}
library(plyr)
library(tidyverse)

library(knitr)
library(ggthemes)
library(stringr)
library(VIM)
library(car)
library(gridExtra)


```






## Dataset load into R

We upload the datasets into R.

```{r ,message=FALSE, warning=FALSE}

season_stats <- read.csv('nba-players-stats/Seasons_Stats.csv')

salaries <- read.csv('salaries.csv')


```



# Dataset description


Two datasets are used:

* `season_stats`: Dataset containing advanced stats since the year 1950. It was obtained through [*this link*](https://www.kaggle.com/drgilermo/nba-players-stats). A glossary with an explanation of the attributes of this dataset can be cheked [here](https://www.basketball-reference.com/about/glossary.html).

* `salaries`: Dataset containing the salaries of the NBA players from 1990 until 2018. Accessed via [*this link*](https://www.kaggle.com/whitefero/nba-player-salary-19902017/downloads/nba-player-salary-19902017.zip/1). 


Now let us describe briefly both datasets.


The first dataset has 24691 observations each one with 53 variables.
The second one, contains 11837 observations of  7 variables.

That amount of data is too big for the purpose of this analysis, so we will reduce the dimensions of both datasets. The idea is to join both datasets to get a full dataset with player stats and salaries. After that, we will select only the data from the 2016-17 season. We will also use the salaries of the 2017-18 season.

The remaining data will not be considered in the main analyses, though in the last section, it will be used to make some visualizations.


Also, some variable selection will be made. We will choose 23 of the variables from the first dataset and only one from the second.

The selected variables are the following. From the first dataset:
 <!-- * `X`: Number of column in the dataset   -->
 <!-- * `Year`: Year that the season occurred. Since the NBA season is split over two calendar years, the year given is the last year for that season. For example, the year for the 1999-00 season would be 2000.     -->
 
 * `Player`: Name of the player.    
 
 * `Pos`: Position of the player. It can be one of the following 5: PG, SG, SF, PF, C, or a combination of those.     
 
 * `Age`: Age of the player.    
 
 * `Tm`: Team of the player.  
 
 * `G`: Games played by the player on that season.     
 
 * `GS`: Games started by the player on that season, i.e., games where the player was on the starting line-up.  
 
 * `MPG`: Minutes Played per game.  
 
 * `PPG`: Points scored per game.  
 
 * `APG`: Assists made per game.   
 
 * `RPG`: Rebounds per game.   
 
 * `SPG`: Steals per game.
 
 * `BPG`:  Blocks per game.
 
 * `TOPG`: Turnovers per game.
 
 * `PFPG`:  Personal fouls per game.
 
 * `PER`:  Player Efficiency Rating (available since the 1951-52 season); PER is a rating developed by ESPN.com columnist John Hollinger. In John's words, "The PER sums up all a player's positive accomplishments, subtracts the negative accomplishments, and returns a per-minute rating of a player's performance."  
 
 * `TS.`: TS%, True Shooting Percentage, a measure of shooting efficiency that takes into account field goals, 3-point field goals, and free throws.   

<!-- *`X3PAr`:  3-Point Field Goal Attempts (available since the 1979-80 season in the NBA). -->
  
 * `USG.`: Usage Percentage (available since the 1977-78 season in the NBA). Usage percentage is an estimate of the percentage of team plays used by a player while he was on the floor.   
   
 * `WS`: an estimate of the number of wins contributed by a player.      
   
 * `VORP`: Value Over Replacement Player (available since the 1973-74 season in the NBA); a box score estimate of the points per 100 TEAM possessions that a player contributed above a replacement-level (-2.0) player, translated to an average team and prorated to an 82-game season   
   
 * `FG.`: Field Goal Percentage. 
 
 * `X2P.`: 2-Point Field Goal Percentage.  
 
 * `X3P.`: 3-Point Field Goal Percentage (available since the 1979-80 season in the NBA).   
 
 <!-- *`X3PA`: 3-Point Field Goal Attempts (available since the 1979-80 season in the NBA).      -->
   
   
    
And from the second dataset: 

* `Salary.in..`: The amount of money that the players gets for the season, in dollars.




## Importance of the dataset and questions that will be studied

There are three main questions that we would like to answer:

* Are the Win Shares the factor that is more correlated to the salary of a player? 

* Is there any significant difference in any of the main statistics for the different five positions of the game? 

* Can the players' salaries be predicted?

To answer these questions, we will work with the data from the 2016-17 season, as it is the last for which there is data on both salaries and in-game stats.

Furthermore, we try to answer some small questions with the help of some plots. The questions that we pose are the following:

* How did the number of points scored evolved through the years?

* Which teams are the ones which expent the most on salaries? Were they successfull teams?  
* Was there any change in the number of three points attempts in the history of the league?  

# Data cleaning


We start with some basic cleaning of the datasets. First, let us remove two variables that are empty and will not be used.

```{r}
# remove blanl & blank2: empty variables

season_stats <- season_stats %>% select(-c(blanl, blank2))


```

Next, we will clean the salary variable, by removing the dollar sign and changing the variable type to numeric. For that, we have to take care of the decimal separator and the commas
```{r}

# remove $ sign
salaries$Salary.in.. <- str_replace_all(salaries$Salary.in.., fixed("$"), "")

# remove point as it confuses the conversion to numeric
salaries$Salary.in.. <- str_replace_all(salaries$Salary.in.., fixed("."), "")

# replace the decimal separator from comma to dot
salaries$Salary.in.. <- str_replace_all(salaries$Salary.in.., fixed(","), ".")

# convert the salaries to numeric
salaries$Salary.in.. <- as.numeric(salaries$Salary.in..)


```


We now restrict the datasets to the 2016-17 season, as it is the one which has more interest to our analysis (we want to study the statistics for the las available season, that is, 2016-17).

```{r, warning=FALSE}
# year 2017: season 16_17

salaries_2017 <- salaries %>% filter(Season.End==2017)

season_stats_2017 <- season_stats %>% filter(Year==2017)

# update the levels in the subset 2017

#stats
season_stats_2017$Pos <- factor(season_stats_2017$Pos)
season_stats_2017$Player <- factor(season_stats_2017$Player)
season_stats_2017$Tm <- factor(season_stats_2017$Tm)

#salaries
salaries_2017$Player.Name <- factor(salaries_2017$Player.Name)
salaries_2017$Team <- factor(salaries_2017$Team)


# update and match the levels on both teams
salaries_2017$Team <- revalue(salaries_2017$Team, c("CHA"="CHO", "NJN"="BRK", "NOH" = "NOP"))

```


<!-- Let us check the structure of the datasets. -->

<!-- ```{r} -->
<!-- str(season_stats_2017) -->

<!-- str(salaries_2017) -->
<!-- ``` -->


## Zeroes and NAs handling

We start with the NA handling for both datasets. The salaries_2017 dataset doesn't contain any null value, as it is shown below.

```{r}
salaries_2017 %>%
  select(everything()) %>%  
  summarise_all(funs(sum(is.na(.))))
```

The other dataset does have some null values that should be analyzed.

```{r}
# number of NAs
season_stats_2017 %>%
  select(everything()) %>%  
  summarise_all(funs(sum(is.na(.))))


```

Now we should think if those values should be imputed or else if we can drop them from the dataset.
Let us examine those samples which have some missing values and then decide what is more suitable for each case.

```{r}
### lists--NA

# residual: players who didn't play much
season_stats_2017 %>% 
  filter(is.na(FT.) == TRUE) %>% 
  select(c("Player", "Tm", "Pos", "G", "MP")) %>%
  arrange(desc(G)) %>%
  head(n=10)

# important: some big stars included
season_stats_2017 %>% 
  filter(is.na(X3P.) == TRUE) %>% 
  select(c("Player", "Tm", "Pos", "G", "MP")) %>%
  arrange(desc(G)) %>%
  head(n=10)

```

While the first case is residual, as only players with few minutes played through the season appear, the second case is more important, as it contains some good overall players, like Capela or Whiteside, and others who logged a big number of minutes.

For the first case, we will drop the values and for the second we will impute them.

```{r}
#drop values with NA values in FT.
season_stats_2017<- season_stats_2017 %>% drop_na(FT.)
```

Let us see how many NAs are still present on the dataset:

```{r}
season_stats_2017 %>%
  select(everything()) %>%  
  summarise_all(funs(sum(is.na(.))))
```


We will see which are the samples with NAs in `X2p.`

```{r}
# check X2P.
season_stats_2017 %>% 
  filter(is.na(X2P.) == TRUE) %>% 
  select(c("Player", "Tm", "Pos", "G", "MP")) %>%
  arrange(desc(G))

```

It is just one player who didn't play many minutes, so we will drop this sample from the dataset.
```{r}
# drop the sample
season_stats_2017<- season_stats_2017 %>% 
  drop_na(X2P.)
```


Next, we deal with the imputation of the NAs values of the variable `X3P.`.

```{r}
season_stats_2017 %>% 
  filter(is.na(X3P.) == TRUE) %>% 
  select(c("Player", "Tm", "Pos", "G", "MP", "X3P", "X3PA")) %>%
  arrange(desc(X3P)) %>%
  head(n=10)
```


We see that those values are empties because those players did not shoot any 3-pointer through the entire season. Thus, it would make sense not to consider the variable for them. 
To simplify the analysis, we can imput the value 0. If they didn't shoot any 3, they probably would have a bad percentage anyway.

```{r}
#replace NAs by 0
season_stats_2017 <- season_stats_2017 %>% 
  mutate(X3P. = replace_na(X3P., 0) )

##check the results on two of the players
season_stats_2017 %>% 
  filter(Player %in% c("Clint Capela", "Hassan Whiteside")) %>% 
  select(c(Player, X3P. ))

```


## Computing Per Game statistics

We will compute some basic per game statistics that are not included in the `season_stats` dataset, namely, points per game (PPG), assiste per game (APG), rebounds per game (RPG), blocks per game (BPG), steals per game (SPG), turnovers per game (TOPG), personal fouls per game (PFPG) and minutes per game (MPG). Those variables will be used in the following analyses and are more convenient to understand them and to compare the values of the different players.

```{r}
season_stats_2017 <- season_stats_2017 %>%
  mutate(PPG = PTS/G, APG = AST/G, RPG = TRB/G, 
         BPG = BLK/G, SPG = STL/G, MPG = MP/G, 
         TOPG = TOV/G, PFPG = PF/G ) 

```



## Join the two tables
Finally, we can create the table that resulsts from the union of the stats of 2017 table and the 2017 salaries.

```{r, warning=FALSE}
# join stats with salaries: join by player and team to avoid duplicities of players

salaries_2017_join <- salaries_2017 %>% 
  select(c(Player.Name, Team, Salary.in..))

stats_with_salaries <- inner_join(season_stats_2017, salaries_2017_join,
                                  by = c('Player' = 'Player.Name', "Tm"="Team"))


head(stats_with_salaries[,c(1:10,53)])

```


Now let us analyze if there were any NAs created with this process.

```{r}
stats_with_salaries %>%
  select(everything()) %>%  
  summarise_all(funs(sum(is.na(.))))
```

We see that no null values were introduced.

## Subset selection

After cleaning the data, we will select a subset of all the variables, with which we will continue the analysis.
Nevertheless, the data cleaning made in the previous section can be useful for further analyses on the dataset.

```{r}

subset <- stats_with_salaries %>%
  select(c(Player,Tm, Pos, Age,  G, GS, MPG, PPG, APG, RPG, BPG,
           SPG, TOPG, PFPG, WS, PER, VORP, X2P., X3P., FG., TS., 
           USG., Salary.in.. ) )

# variables that we will keep
names(subset)

```



## Outliers

Now we can check for outliers on every variable of the previously selected ones.

```{r}
boxplot.stats(subset$Age)$out

boxplot.stats(subset$G)$out


boxplot.stats(subset$MPG)$out


boxplot.stats(subset$PPG)$out


boxplot.stats(subset$APG)$out


boxplot.stats(subset$RPG)$out


boxplot.stats(subset$BPG)$out


boxplot.stats(subset$SPG)$out



boxplot.stats(subset$TOPG)$out


boxplot.stats(subset$PFPG)$out


boxplot.stats(subset$WS)$out

boxplot.stats(subset$PER)$out


boxplot.stats(subset$VORP)$out


boxplot.stats(subset$X2P.)$out

boxplot.stats(subset$X3P.)$out

boxplot.stats(subset$FG.)$out


boxplot.stats(subset$TS.)$out


boxplot.stats(subset$USG.)$out


boxplot.stats(subset$Salary.in..)$out


```


We see that all of them correspond to different players, so those are possible values that we can leave as they are.



# Data analysis
In this section, we will state three questions that we want to answer with the dataset that we constructed in the previous section. Then, analyses will be made in order to answer the questions.

First, we start by some preliminary exploratory analysis.

## Exploratory analysis

Let us explore the players with the highest values of some of the statistics: MPG, PPG, APG, RPG, TOPG, PFPG and Salary. We will output the 10 players with the higher values for the statistics.

 

```{r}

# MPG

subset %>% 
  select(c(Player, Pos, MPG )) %>% 
  arrange(desc(MPG)) %>% 
  head (n=10)
```



```{r}

# PPG

subset %>% 
  select(c(Player, Pos, PPG )) %>% 
  arrange(desc(PPG)) %>% 
  head (n=10)
```

```{r}

# APG

subset %>% 
  select(c(Player, Pos, APG )) %>% 
  arrange(desc(APG)) %>% 
  head (n=10)
```

```{r}

# RPG

subset %>% 
  select(c(Player, Pos, RPG )) %>% 
  arrange(desc(RPG)) %>% 
  head (n=10)
```


```{r}
# TOPG
subset %>% 
  select(c(Player, Pos, TOPG )) %>% 
  arrange(desc(TOPG)) %>% 
  head (n=10)
```



```{r}
# PFPG

subset %>% 
  select(c(Player, Pos, PFPG)) %>% 
  arrange(desc(PFPG)) %>% 
  head (n=10)
```

```{r}
# Salary

subset %>% 
  select(c(Player, Pos, Salary.in.. )) %>% 
  arrange(desc(Salary.in..)) %>% 

  head (n=10)
```


## Are the Win Shares the factor that is more correlated to the salary of a player?

We start with the hypothesis that whe Win Shares (WS) is the factor that contributes the most to a player's salary. Let's see if that hypothesis is true. 

If not, we will examine some other factors to determine which is the one more related to the salary of a player.

First, let's check who are the 30 players with the higher WS and see how much they are getting paid

```{r}
# 30 players with higher WS
subset %>% 
  select(c(Player, Tm, Pos,  WS,  Salary.in.. )) %>%
  arrange(desc(WS)) %>%
  head(n=30)


```

In general, those players have high salaries, except for some exceptions like Antetokounmpo or Jokic, who probably were on their rookie deals.

As a curiosity, let's calculate the Win shares for the teams:

```{r}
#teams with more WS
stats_with_salaries %>%
  group_by(Tm) %>%
  summarise( sum_WS = sum(WS), sum_salary = sum(Salary.in..)) %>%
  select(c(Tm, sum_WS, sum_salary ))  %>%
  arrange(desc(sum_WS))
```


We see that it is almost identical to the number of victories of the team that season.


### Normality and homogeneity check for the variance
We want to check the correlation between the WS and the salaries. We will check if the variables are normally distributed and homocedastic

```{r}
#normality test shapiro-wilk WS and salaries

shapiro.test(stats_with_salaries$WS)
shapiro.test(stats_with_salaries$Salary.in..) 

#homocedasticity

fligner.test(WS ~ Salary.in.., data = subset) 

```

### Computing the correlation

We saw that the variables are normal but there isn't homocedasticity, so the Pearson correlation cannot be applied. Instead, we shall apply the Spearman correlation for these two variables. Let us see the results:



```{r}
# correlation: WS and Salary
cor(stats_with_salaries$WS, stats_with_salaries$Salary.in.., method = 'spearman')
```

The result is a correlation of $0.58$. There is some positive correlation, but is not a great one. Let's check some other variables, to see if there is another one which is more positively correlated to the salary:

```{r}
subset1 <- stats_with_salaries %>% 
  select(c(Player, Tm, Pos, Age, TS., WS, PER, G, 
           PPG, APG, RPG, BPG, SPG, TOPG, MPG, Salary.in.. ))

cor(subset1$Salary.in.., subset1[4:15], method="spearman")
```

We see that the variable that is more positively correlated to the salary is MPG, closely followed by PPG. 

It can be concluded that, usually, the players that earn the most are also the ones that play more minutes per game and the ones that score more points per game. This makes sense, as if you have to spend more money on a certain player, you would expect him to ve valuable for the team and thus make him play more minutes and allow him to shoot more often. 

The teams value more the points and the minutes, rather than some advanced metrics like the PER, or the True Shooting %. Nevertheless, the metric of Win Shares follows closely the PPG and MPG in correlation with salary.

Let's compute the correlation between salaries and FG.(field goal %) to check if the players who are getting paid the most are also the ones who shoot more efficiently.

```{r}
cor.test(subset$FG., subset$Salary.in.., method = 'spearman',  exact=F)


```

The correlation is not so high, so being an efficient scorer seems not to be highly correlated with a higher salary.

--------------------------


## Is there any significant difference in any of the main statistics for the different five positions of the game?

Let's check if there are any differences in some of the different stats for the five player positions. If any differences are found, We will perform different statistical tests to check if those differences are significative or not.



### Analysis planification


```{r}
subset2 <- stats_with_salaries %>% 
  select(c(Player, Tm, Pos, Age, WS, PPG, APG, MPG, Salary.in.. ))
  #select(c(Player, Tm, Pos, Age, TS., WS, PER, G, PPG, APG, RPG, BPG, SPG, MPG, Salary.in.. ))
```

We select the five analysis groups, i.e., the five types of players in the game. 
Let us see a sample of these groups.

```{r}

group.PG <- subset2 %>%  filter(Pos == 'PG')

head(group.PG)


group.SG <- subset2 %>%  filter(Pos == 'SG')

head(group.SG)

group.SF <- subset2 %>%  filter(Pos == 'SF')

head(group.SF)

group.PF <- subset2 %>%  filter(Pos == 'PF')

head(group.PF)

group.C <- subset2 %>%  filter(Pos == 'C')

head(group.C)

```

We are going to study if there are any significative differences in the following variables, depending on the position of the player:

* Salary  
* Points per Game  
* Assists per Game  
* Minutes per Game  
* Win shares  

First of all, we must check if each of those variables are normally distributed and if there is homocedasticity. If the answer is affirmative, then an ANOVA test can be performed. If the answer is negative, we should perform a non-parametric test like the Kruskal-Wallis.



#### Salary
```{r}
# normality test

shapiro.test(subset2$Salary.in..)

```


```{r}
# homocedasticity test
leveneTest(Salary.in..  ~ Pos, data = subset2)
```


The variable is normally distributed, but there is no homocedasticity. Thus, we will apply the Kruskal-Wallis test to see if there are any differences with respect to the Salary by position.

```{r}

test_salary <- kruskal.test(Salary.in..~ Pos, data= subset2)
test_salary

```

There is no evidence that there exist a difference on the salary by the different position that a player has.

#### PPG
```{r}

# normality test

shapiro.test(subset2$PPG)

```

```{r}
# homocedasticity test

leveneTest(PPG  ~ Pos, data = subset2)
```


The variable is normally distributed, but there is no homocedasticity. Thus, we will apply the Kruskal-Wallis test to see if there are any differences with respect to the PPG by position.

```{r}

test_PPG <-  kruskal.test(PPG~ Pos, data= subset2)

test_PPG
```

After running the test, no evidence was found that the PPG change significatively according to the player's position on the field.


#### APG
```{r}
# diferencias en APG

shapiro.test(subset2$APG)

```

```{r}
# homocedasticity test

leveneTest(APG  ~ Pos, data = subset2)
```

The variable is normally distributed and there is homocedasticity, so we can apply the ANOVA test to see if there are any differences with respect to the APG by position.

```{r}

# normal--- we can do the anova test
test_APG <- aov(APG~ Pos, data= subset2)
summary(test_APG)
summary(test_APG)[[1]]$'Pr(>F)'[1]
```

In this case, the Assists per Game depend on the position that the player has on the field. The p-value is really small (less than $10^{-16}$).
Later, we will explore which of the possitions has a higher number of APG.


#### MPG

```{r}
# normality test

shapiro.test(subset2$MPG)
```

```{r}
# homocedasticity test
leveneTest(MPG  ~ Pos, data = subset2)
```

The variable is normally distributed, but there is no homocedasticity. Thus, we will apply the Kruskal-Wallis test to see if there are any differences with respect to the MPG by position.

```{r}

test_MPG <- kruskal.test(MPG~ Pos, data= subset2)
test_MPG

```

In the case of the MPG, we found that there is evidence to support the claim that the minutes played depend on the position. In this case, the p-value is still small (in the order of $10^{-3}$).

#### WS
```{r}
# normality test

shapiro.test(subset2$WS)
```

```{r}
leveneTest(WS  ~ Pos, data = subset2)
```

The variable is normally distributed and there is homocedasticity, so we can apply the ANOVA test to see if there are any differences with respect to the WS by position.

```{r}

# normal--- we can do the anova test
test_WS <- aov(WS~ Pos, data= subset2)
summary(test_WS)
```

In this case, the WS also have some dependence on the position played, but the p-value was closer to the significance level ($0.009$) than in the other two variables, meaning that further investigations should be made in order to determine whether there is a true effect or just a result of chance.



Let us see a summary of the results of the analyses.

```{r}
DT = data.frame(
  Variable = c("Salary","PPG","APG","MPG","WS"),
  Result = c("No significative difference observed",
             "No significative difference observed",
             "Significative difference",
             "Significative difference",
             "Significative difference"),
  p.value = c(sprintf("%0.3f", test_salary$p.value),
            sprintf("%0.3f", test_PPG$p.value),
            sprintf("%0.2e", summary(test_APG)[[1]]$'Pr(>F)'[1]),
            sprintf("%0.2e", test_MPG$p.value),
            sprintf("%0.3f", summary(test_WS)[[1]]$'Pr(>F)'[1]))
)

DT
```


### Further analysis with WS, APG and MPG.

Now, let us check the source of the differences in the variables WS, APG and MPG are coming from, i.e., which possitions have a higher value for those variables.

#### WS

```{r}
# WS
summary(lm(WS ~ Pos, data = subset2))
```

```{r , fig.width = 3.5, fig.height = 3}
subset2 %>% 
  ggplot(aes(x = Pos, y = WS)) + 
  geom_boxplot()+
  theme_classic()
```

It seems that the positions with the higher WS are the Centers, followed by the Small Forwards.

#### APG

```{r}
# APG
summary(lm(APG ~ Pos, data = subset2))
```

We can plot those differences in a boxplot:

```{r, fig.width = 3.5, fig.height = 3}
subset2 %>% 
  ggplot(aes(x = Pos , y = APG)) + 
  geom_boxplot()+
  theme_classic()
```


```{r}
pairwise.t.test(subset2$APG, subset2$Pos)
```

The results of the pairwise t-test mean that the PG has significant differences in APG with every other position in the game, while the other positions (except for SG with PF and C) do not have significant differences betweent each other.

It is quite clear that the PG are the ones with the higher APG.

#### MPG
Let us repeat the analysis for the MPG attribute.

```{r}
# MPG
summary(lm(MPG ~ Pos, data = subset2))
```

```{r , fig.width = 3.5, fig.height = 3}
subset2 %>% 
  ggplot(aes(x = Pos, y = MPG)) + 
  geom_boxplot()+
  theme_classic()
```


It seems that the Small Forwards are getting more minutes per game. Let us see how many players are in the league for every position in the 2016-17 season.

```{r}
summary(subset$Pos)
```

We see that the SF is the position with less players, which can be the reason that is the position with the higher MPG.


-------------------------
## Salary prediction model

We are going to use the following subset of variables to predict the salary:

```{r}
subset3 <- subset %>%
  select(-c(Tm, Player))

names(subset3)
```


### Models creation
Let us create different models and compare their adjusted R-squared as an indicator of which model will make a better adjustment to the data.

Let us first use a model which uses all the variables in the subset. Let us display the summary and make some comments about it.

```{r}
# all variables
model1 <- lm(Salary.in.. ~ ., 
             data=subset3)

summary(model1)
```

We see that some of the variables that seem to explain better the salary are the Position, the Games Started, PPG, RPG and PFPG.

Let us create a plot to see how the variables with the most significant p-value from the previous output (those are: PPG, MPG, RPG, GS, PFPG and AGE) are related to the Salary. The following code gives the desired plot.

```{r ,message=FALSE, warning=FALSE}


# Salary vs PPG
p1 <- ggplot(subset, aes(x=Salary.in.., y=PPG)) +
  geom_point(alpha=0.5) + geom_smooth() +
  labs(x="Salary", y="Points PG") +
  theme(legend.position="none") +
  scale_x_continuous(breaks = c(100000, 10000000,
                                20000000, 30000000),
                     labels = c("$1M", "$10M",
                                "$20M", "$30M"))+
  theme_classic() +
  theme(
    plot.margin = margin(3, 7, 3, 1.5)
  )


# Salary vs MPG
p2 <- ggplot(subset, aes(x=Salary.in.., y=MPG)) +
  geom_point(alpha=0.5) + geom_smooth() +
  labs(x="Salary", y="Minutes PG") +
  theme(legend.position="none")+
  scale_x_continuous(breaks = c(100000, 10000000,
                                20000000, 30000000),
                     labels = c("$1M", "$10M",
                                "$20M", "$30M"))+
  theme_classic() +
  theme(
    plot.margin = margin(3, 7, 3, 1.5)
  )

# Salary vs GS
p3 <- ggplot(subset, aes(x=Salary.in.., y=GS)) +
  geom_point(alpha=0.5) + geom_smooth() +
  labs(x="Salary", y="Games Started") +
  theme(legend.position="none")+
  scale_x_continuous(breaks = c(100000, 10000000,
                                20000000, 30000000),
                     labels = c("$1M", "$10M",
                                "$20M", "$30M"))+
  theme_classic() +
  theme(
    plot.margin = margin(3, 7, 3, 1.5)
  )


# Salary vs PFPG
p4 <- ggplot(subset, aes(x=Salary.in.., y=PFPG)) +
  geom_point(alpha=0.5) + geom_smooth() +
  labs(x="Salary", y="Personal Fouls PG") +
  theme(legend.position="none")+
  scale_x_continuous(breaks = c(100000, 10000000,
                                20000000, 30000000),
                     labels = c("$1M", "$10M",
                                "$20M", "$30M"))+
  theme_classic() +
  theme(
    plot.margin = margin(3, 7, 3, 1.5)
  )

# Salary vs RPG
p5 <- ggplot(subset, aes(x=Salary.in.., y=RPG)) +
  geom_point(alpha=0.5) + geom_smooth() +
  labs(x="Salary", y= "Rebounds PG") +
  theme(legend.position="none")+
  scale_x_continuous(breaks = c(100000, 10000000,
                                20000000, 30000000),
                     labels = c("$1M", "$10M",
                                "$20M", "$30M"))+
  theme_classic() +
  theme(
    plot.margin = margin(3, 7, 3, 1.5)
  )

# Salary vs Age
p6 <- ggplot(subset, aes(x=Salary.in.., y=Age)) +
  geom_point(alpha=0.5) + geom_smooth() +
  labs(x="Salary", y="Age") +
  scale_x_continuous(breaks = c(100000, 10000000,
                                20000000, 30000000),
                     labels = c("$1M", "$10M",
                                "$20M", "$30M"))+
  theme_classic() +
  theme(
    plot.margin = margin(3, 7, 3, 1.5)
  )

# Represent all the subplots 
grid.arrange(p1, p2, p3, p4, p5, p6,
             layout_matrix=cbind(c(1,4), c(2,5), c(3,6)), 
             top = textGrob("Predictors related to the Salary",
                            gp=gpar(fontsize=20,font=3)))
```

It seems that the variables whose correlation is more clear with the salary are MPG, PPG and GS.

Let us build some more linear regression models. Then, we will select the better performing one.

```{r}
model2 <- lm(Salary.in.. ~ APG + RPG + PPG + Age + BPG + MPG*PPG 
             + Age*MPG, 
             data=subset3)

```


```{r}
model3 <- lm(Salary.in.. ~ MPG +  RPG + Age + BPG + MPG*PPG + Age*MPG,
             data=subset3)

```

```{r}
model4 <- lm(Salary.in.. ~  Age*MPG + WS + TOPG + PFPG + GS,
             data=subset3)

```


```{r}
model5 <- lm(Salary.in.. ~  I(Age)^2 + I(PPG)^3+ Age*MPG +
               Pos + RPG + TS. + GS + APG*TOPG+ USG.,
             data=subset3)

```


```{r}
model6 <- lm(Salary.in.. ~  I(Age)^2 + I(PPG)^3+  I(PFPG)^2+ 
               Age*MPG + Pos + I(RPG)^2 + TS. + I(GS)^2 + APG*TOPG+ 
               USG.*TOPG + VORP + WS, data=subset3)

```

We print the results in the following table. We include the F-statistic as an indicator of wheter there is a relationship between the Response and the Predictors (the larger the F-statistic, the more relationship there is).

```{r}
models.table <- 
  data.frame(c(1, 2, 3 ,4 ,5, 6),
                           c(summary(model1)$adj.r.squared,
                             summary(model2)$adj.r.squared,
                             summary(model3)$adj.r.squared,
                             summary(model4)$adj.r.squared,
                             summary(model5)$adj.r.squared,
                             summary(model6)$adj.r.squared),
                           c(summary(model1)$fstatistic[1],
                             summary(model2)$fstatistic[1],
                             summary(model3)$fstatistic[1],
                             summary(model4)$fstatistic[1],
                             summary(model5)$fstatistic[1],
                             summary(model6)$fstatistic[1]))

names(models.table) <- c("Model", "Adj. R-squared", "F-statistic")

models.table
```

It seems that the model number 6 has the highest adjusted R-squared, so that is the one that we will use. But we will also keep an eye on the model number 3, the one with the higher F-statistic.


### What salary would the model predict for Free Agent players? 

Now, let us now predict the salaries that some of the free agents of the 2016-2017 season would make in the season 2017-2018, according to the best performing model from the previous section. We can check the list of free agents in this [link](https://www.hoopsrumors.com/2017/06/top-50-nba-free-agents-of-2017.html).

To make this test, we will choose the following players:

* Blake Griffin. 

* Steph Curry.   

* Kyle Lowry.  

* George Hill.  

* Serge Ibaka.  

* Pau Gasol.

* J.J. Redick.


To make the predictions, we will use the model named `model6`, as it was the one with the higher adjusted R squared of all the models built in the previous section.

```{r, warning=FALSE}
griffin <- subset %>% filter(Player == 'Blake Griffin')
# predicted salary
pred_griffin <- predict(model6, griffin)

curry <- subset %>% filter(Player == 'Stephen Curry')
# predicted salary
pred_curry <- predict(model6, curry)

lowry <- subset %>% filter(Player == 'Kyle Lowry')
# predicted salary
pred_lowry <- predict(model6, lowry)

hill <- subset %>% filter(Player == 'George Hill')
# predicted salary
pred_hill <- predict(model6, hill)

ibaka <- subset %>% filter(Player == 'Serge Ibaka')
# predicted salary
pred_ibaka <- predict(model6, ibaka)

p.gasol <- subset %>% filter(Player == 'Pau Gasol')
# predicted salary
pred_gasol <- predict(model6, p.gasol)

redick <- subset %>% filter(Player == 'J.J. Redick')
# predicted salary
pred_redick <- predict(model6, redick)

preds_model6 <- c(pred_griffin, pred_hill, pred_redick, 
                  pred_lowry, pred_gasol, pred_ibaka, pred_curry)

```


Now we can compare those predictions with the actual salaries those players got in that season. These values are included in the `salaries` dataset. We extract that information and compare the predictions with the true values in the following lines of code.


```{r, warning=FALSE}

player_names <- c('Blake Griffin', 'Stephen Curry', 'Kyle Lowry', 
             'George Hill', 'Serge Ibaka', 'Pau Gasol', 'J.J. Redick' )

# see the filtered players
subset %>% filter(Player %in% player_names)

# salaries in 2016-17
true_salaries <-
  salaries_2017 %>% 
  filter(Player.Name %in% player_names) %>%
  select(Player.Name, Salary.in..)
names(true_salaries) <- c("Player", "salary_2017")

#predicted sal. for 2017-18 for these players
true_salaries$predicted_2018 <- preds_model6 

# salaries they obtained in the 2017-18 season
true_salaries_2018 <-
  salaries %>% 
  filter(Season.End==2018, Player.Name %in% player_names) %>%
  select(Player.Name, Salary.in..)
names(true_salaries_2018) <- c("Player", "true_salary_2018")

# see the differences
salaries_comparison <- full_join(true_salaries, true_salaries_2018, 
                                 by =c('Player' = 'Player' ) )
salaries_comparison$pred_error <- salaries_comparison$true_salary_2018 -
  salaries_comparison$predicted_2018 

salaries_comparison

```

We have quite different results. We can see that for Pau Gasol, the prediction error is less than 1.5 M$, but for the other players, the differences are unacceptable. It is intereseting, though, to see how the model detected a salary raise in the J.J. Redick and George Hill salaries for the 2017-18 season, though the raise was shorter than in the real life. 


### Which predictions would other models make?

We can show the predictions made by the different models, to see if the model with the highest adjusted R-squared was actually the one that made the best predictions. Let us check `model1`, which used all variables and `model3`, the one with the higher F-statistic.


```{r}
# model 1
pred_griffin_model1 <- predict(model1, griffin)

pred_curry_model1 <- predict(model1, curry)

pred_lowry_model1 <- predict(model1, lowry)

pred_hill_model1 <- predict(model1, hill)

pred_ibaka_model1 <- predict(model1, ibaka)

pred_gasol_model1 <- predict(model1, p.gasol)

pred_redick_model1 <- predict(model1, redick)

preds_model1 <-  c(pred_griffin_model1, pred_hill_model1, pred_redick_model1,
                  pred_lowry_model1, pred_gasol_model1, pred_ibaka_model1, 
                  pred_curry_model1)

# model 3
pred_griffin_model3 <- predict(model3, griffin)

pred_curry_model3 <- predict(model3, curry)

pred_lowry_model3 <- predict(model3, lowry)

pred_hill_model3 <- predict(model3, hill)

pred_ibaka_model3 <- predict(model3, ibaka)

pred_gasol_model3 <- predict(model3, p.gasol)

pred_redick_model3 <- predict(model3, redick)

preds_model3 <- c(pred_griffin_model3, pred_hill_model3, pred_redick_model3,
                  pred_lowry_model3, pred_gasol_model3, pred_ibaka_model3, 
                  pred_curry_model3)

```




```{r}
true_salaries$predicted_model1 <-  preds_model1
     
true_salaries$predicted_model3 <-  preds_model3
      
# see the differences
salaries_comparison <- full_join(true_salaries, true_salaries_2018, by =c('Player' = 'Player' ) )

salaries_comparison
```




It seems that `model3` is the one making the most accurate predictions overall. Some further tests would be necessary to determine which of the models has more accurate predictions.

### Conclusion
In general, the models that were built in the last sections are too simple to reflect the complexity of the NBA salary system. Some improvements are necessary in order to achieve a higher performing model, that is able to predicto more accurately the salaries.


Some ideas to improve the model would be the following:

 * Change the model from regression to other ones that are able to detect the nature of the salaries. Let us notice that the best adjusted R-squared obtained was around 0.6, which is not a great value.   

 * There are other considerations in the salary predictions that were not considered. For example, **the salary cap available for a season** (that is, the limit of money that every team can spend on salaries) or the **player's eligibility for a super-max contract**. In the case of Steph Curry, he was eligible for a super-max extension, thus, he earned a lot more money than predicted.  

 * If a player enters free-agency from a rookie contract, he is expected to earn more money the next season. This wasn't considered in this model.  

# Data visualizations

In this section, some data visualizations are included, aiming to answer some small questions about the evolution of the league in the last years.


## How many points were scored each season through the NBA history?

```{r,message=FALSE, warning=FALSE}

## evolution of points scored though the years------------------------

# group points by year

points_by_year <- season_stats %>% 
  group_by(Year) %>% 
  summarise(total = sum(PTS))

str(points_by_year)

# plot; geom_smooth options: method = "lm", se = FALSE

points_by_year %>% 
  ggplot(aes(Year, total)) + 
  geom_point()+ 
  geom_smooth()+ 
  theme_classic()+
  scale_y_continuous(breaks = c(50000, 100000,
                                150000, 200000,
                                250000, 300000),
                     labels = c("50k", "100k", "150k", 
                                "200k", "250k", "300k"))+  
  ggtitle('Evolution in points scored per season') +
  labs(subtitle = "The number of points scored has increased")+
  ylab("Total points scored per season (in thousand points)")


```


## Which are the teams that spent the most money on players' salaries?

```{r}
### averaged salaries grouped by teams -----------
library(tidyverse)

champions_list <- c("LAL", "CLE", "DET", "GSW", "BOS", 
                    "MIA", "CHI", "SAS", "HOU", "DAL")

n_seasons_salaries <- n_distinct(salaries$Season.Start)
salaries_by_team <- salaries %>% group_by(Team) %>% 
  summarise(total = sum(Salary.in..), avg = sum(Salary.in..)/n_seasons_salaries) %>%
  mutate(highlight_flag = ifelse(Team %in% champions_list, T, F))



# plot: champion teams are displayed in red colour 
salaries_by_team %>% 
  ggplot(aes(x = reorder(Team, avg), avg)) + 
  geom_bar(stat = "identity", 
           aes(fill = highlight_flag)) +
  scale_fill_manual(values = c('#595959', 'red')) +
  theme_classic() +
  ggtitle('Average money spent on salaries by each team (from 1990 to 2017)') +
  labs(subtitle = "Spending more money does not guarantee championships") + 
  theme(axis.text.y = element_text(size=8),
        axis.text.x = element_text(size=10),
        legend.position = 'none') +
  scale_y_continuous(breaks = c(20000000, 40000000, 60000000),
                     labels = c("$20M", "$40M", "$60M"))+
  ylab("Avg. money spent on salary per season (in $M)") +
  xlab("Team (champion teams are highlighted in red)")+
  coord_flip()

                     
```


## Was there an evolution in the number of three point attempts through the NBA history?
```{r,message=FALSE, warning=FALSE}

## evolution of 3pts attemps though the years------------------------

# group 3 pts by year :after 1982, when first collected data

threes_att_by_year <- season_stats %>% filter(Year>1982) %>%
  group_by(Year) %>% 
  summarise(total = sum(X3PA))

str(threes_att_by_year)

# plot; geom_smooth options: method = "lm", se = FALSE

threes_att_by_year %>% ggplot(aes(Year, total)) + 
  geom_point()+ geom_smooth()+ 
  #geom_bar(stat = "identity") + 
  scale_y_continuous(breaks = c(20000, 40000,
                                60000, 80000))+  
  ggtitle('Evolution in three pointes attempted by year') +
  theme_classic()+
  labs(subtitle = "The overall tendence is an increase in the number of attempts")+
  ylab('Total 3pt attempts')



```


# Conclusions 


In this document we explored three hypothesis about the NBA stats and salaries datasets. First, we tried to determine if the Win Shares are the factor that is more correlated to the salary of a player. It was concluded that the variable that is more positively correlated to the salary is MPG, closely followed by PPG. 

The reason why this happens is that, usually, the players that earn the most are also the ones that play more minutes per game and the ones that score more points per game. This makes sense, as if you have to spend more money on a certain player, you would expect him to ve valuable for the team and thus make him play more minutes and allow him to shoot more often. 

Second, we wondered if there there is a significant difference in any of the main statistics for the different five positions of the game. The statistics we have checked are the following: Salary, Points per Game, Assists per Game, Minutes per Game and Win shares.
We found differences by positions in the Assists per game (the PG has the higher average), in Minutes per game (the Small Forwards are getting more minutes per game) and Win Shares (the positions with the higher WS are the Centers, followed by the Small Forwards).

Third, we created a regression model that would predict the salary of a player, given the statistics of the previous season. The conclusion is that some improvements need to be made with the model, in order to be able to obtain more precise predictions.


# Save the clean dataset to .csv

```{r}
write.csv(stats_with_salaries, file = "clean_data.csv")
```

